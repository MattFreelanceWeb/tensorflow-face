{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instalation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow opencv-python tensorflowjs dlib matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de chaque points afin de comprendre comment travailler avec le modèle préentrainé shape predictor 68 \n",
    "\n",
    "les points de repère faciaux pour un visage donné. Chaque point de repère est représenté par une paire de coordonnées (x, y).\n",
    "\n",
    "Les points de repère sont numérotés de 0 à 67, et chaque numéro correspond à une partie spécifique du visage. Voici une liste des points de repère pour un visage avec 68 points :\n",
    "\n",
    "Points 0-16 : Mâchoire\n",
    "Points 17-21 : Sourcil droit\n",
    "Points 22-26 : Sourcil gauche\n",
    "Points 27-30 : Partie supérieure du nez\n",
    "Points 31-35 : Partie inférieur du nez\n",
    "Points 36-41 : Œil droit\n",
    "Points 42-47 : Œil gauche\n",
    "Points 48-59 : Bouche extérieure\n",
    "Points 60-67 : Bouche intérieure\n",
    "\n",
    "- vous pouvez en runnant le script ci dessous apercevoir chaque points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@185.604] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@185.605] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convertir l'image en niveaux de gris\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Détecter les visages dans l'image\u001b[39;00m\n\u001b[1;32m     25\u001b[0m faces \u001b[38;5;241m=\u001b[39m face_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.3\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle de détection de visage pré-entraîné\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Charger le modèle de prédiction de points de repère du visage\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") \n",
    "\n",
    "# Initialiser la webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Initiqliser un point a stocker pour l'utiliser plus tard\n",
    "point_30_coords = 0\n",
    "\n",
    "while True:\n",
    "    # Capture d'une image depuis la webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Détecter les visages dans l'image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Obtenez les points de repère du visage\n",
    "        rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "        landmarks = predictor(gray, rect)\n",
    "\n",
    "        # Convertissez les points de repère en un tableau NumPy\n",
    "        landmarks_np = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
    "\n",
    "        # Stocker les coordonnées du point 30 (nez) dans la variable\n",
    "        point_30_coords = (landmarks_np[30, 0], landmarks_np[30, 1])\n",
    "\n",
    "\n",
    "        # Dessiner le canevas sur l'image\n",
    "        for i in range(landmarks_np.shape[0]):\n",
    "            cv2.circle(frame, (landmarks_np[i, 0], landmarks_np[i, 1]), 3, (0, 255, 0), -1)\n",
    "            cv2.putText(frame, str(i), (landmarks_np[i, 0], landmarks_np[i, 1]), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "\n",
    "        # Relier les points entre eux\n",
    "        for i in range(landmarks_np.shape[0] - 1):\n",
    "            cv2.line(frame, (landmarks_np[i, 0], landmarks_np[i, 1]), (landmarks_np[i + 1, 0], landmarks_np[i + 1, 1]), (0, 255, 0), 1)\n",
    "\n",
    "    print(point_30_coords)\n",
    "\n",
    "    # Afficher le résultat\n",
    "    cv2.imshow('Facial Landmarks', frame)\n",
    "\n",
    "    # Quitter la boucle si la touche 'q' est pressée\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Libérer la webcam et fermer la fenêtre\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import cv2\n",
    "# import dlib\n",
    "# import numpy as np\n",
    "\n",
    "# # Charger le modèle de détection de visage pré-entraîné\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # Charger le modèle de prédiction de points de repère du visage\n",
    "# predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# # Initialiser la webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Charger l'image que vous souhaitez afficher\n",
    "# overlay_image = cv2.imread('smiley.png')  # Remplacez 'votre_image.png' par le chemin de votre image\n",
    "# overlay_image = cv2.resize(overlay_image, (50, 50))  # Ajustez la taille de l'image selon vos besoins\n",
    "\n",
    "# while True:\n",
    "#     # Capture d'une image depuis la webcam\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # Convertir l'image en niveaux de gris\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Détecter les visages dans l'image\n",
    "#     faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         # Obtenez les points de repère du visage\n",
    "#         rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "#         landmarks = predictor(gray, rect)\n",
    "\n",
    "#         # Convertissez les points de repère en un tableau NumPy\n",
    "#         landmarks_np = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
    "\n",
    "#         # Stocker les coordonnées du point 30 (nez) dans la variable\n",
    "#         point_30_coords = (landmarks_np[30, 0], landmarks_np[30, 1])\n",
    "\n",
    "#         # Déplacer et superposer l'image sur le point 30\n",
    "#         overlay_x = int(point_30_coords[0]) - overlay_image.shape[1] // 2\n",
    "#         overlay_y = int(point_30_coords[1]) - overlay_image.shape[0] // 2\n",
    "#         frame[overlay_y:overlay_y + overlay_image.shape[0], overlay_x:overlay_x + overlay_image.shape[1]] = overlay_image\n",
    "\n",
    "#     # Afficher le résultat\n",
    "#     cv2.imshow('Facial Landmarks', frame)\n",
    "\n",
    "#     # Quitter la boucle si la touche 'q' est pressée\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Libérer la webcam et fermer la fenêtre\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "def superposer_image_sur_visage(image_path='alien.png', overlay_factor=1.5):\n",
    "    # Charger le modèle de détection de visage pré-entraîné\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Charger le modèle de prédiction de points de repère du visage\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "    # Initialiser la webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Charger l'image que vous souhaitez afficher avec un fond transparent (au format PNG)\n",
    "    overlay_image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    while True:\n",
    "        # Capture d'une image depuis la webcam\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convertir l'image en niveaux de gris\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Détecter les visages dans l'image\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Obtenez les points de repère du visage\n",
    "            rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "            landmarks = predictor(gray, rect)\n",
    "\n",
    "            # Convertissez les points de repère en un tableau NumPy\n",
    "            landmarks_np = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
    "\n",
    "            # Calculer la largeur et la hauteur de l'image superposée pour couvrir entièrement la tête\n",
    "            overlay_width = int(overlay_factor * w)\n",
    "            overlay_height = int(overlay_factor * h)\n",
    "\n",
    "            # Redimensionner l'image superposée en fonction des nouvelles dimensions\n",
    "            overlay_image_resized = cv2.resize(overlay_image, (overlay_width, overlay_height))\n",
    "\n",
    "            # Définir les coordonnées pour superposer l'image sur la tête\n",
    "            overlay_x = int(x + w/2 - overlay_width/2)\n",
    "            overlay_y = int(y + h/2 - overlay_height/2)\n",
    "\n",
    "            # S'assurer que les coordonnées sont dans les limites de l'image\n",
    "            overlay_x = max(overlay_x, 0)\n",
    "            overlay_y = max(overlay_y, 0)\n",
    "\n",
    "            # Extraire le canal alpha (transparence) de l'image superposée\n",
    "            alpha_channel = overlay_image_resized[:, :, 3] / 255.0\n",
    "\n",
    "            # Appliquer la superposition avec la transparence\n",
    "            for c in range(0, 3):\n",
    "                frame[overlay_y:overlay_y + overlay_height, overlay_x:overlay_x + overlay_width, c] = \\\n",
    "                    (1.0 - alpha_channel) * frame[overlay_y:overlay_y + overlay_height, overlay_x:overlay_x + overlay_width, c] + \\\n",
    "                    alpha_channel * overlay_image_resized[:, :, c]\n",
    "\n",
    "        # Afficher le résultat\n",
    "        cv2.imshow('Facial Landmarks', frame)\n",
    "\n",
    "        # Quitter la boucle si la touche 'q' est pressée\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Libérer la webcam et fermer la fenêtre\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Exemple d'utilisation avec le chemin de l'image par défaut et un facteur de recouvrement de 1.5\n",
    "superposer_image_sur_visage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
